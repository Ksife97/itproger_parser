import requests
from bs4 import BeautifulSoup
import logging
import re
from urllib.parse import urljoin

logger = logging.getLogger(__name__)


class ITProgerParser:
    def __init__(self):
        self.base_url = "https://itproger.com"
        self.news_url = "https://itproger.com/news"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

    def parse_articles(self, page=1):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ URL –¥–ª—è itproger.com
            if page == 1:
                url = self.news_url
            else:
                url = f"{self.news_url}/page-{page}"

            logger.info(f"Parsing URL: {url}")

            response = self.session.get(url)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            articles = []

            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–æ —Å—Ç–∞—Ç—å—è–º–∏ –Ω–∞ itproger.com
            articles_container = soup.find('div', class_='allArticles')

            if not articles_container:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
                articles_container = soup.find('main') or soup.find('div', class_=re.compile(r'content|main'))

            if articles_container:
                # –ò—â–µ–º –≤—Å–µ div —Å –∫–ª–∞—Å—Å–æ–º article –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                article_containers = articles_container.find_all('div', class_='article')
                logger.info(f"Found {len(article_containers)} articles in container")
            else:
                article_containers = []
                logger.warning("No articles container found")

            for container in article_containers:
                article = self._parse_article_card(container)
                if article and article['title'] and article['title'] != "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞":
                    articles.append(article)

            return articles

        except Exception as e:
            logger.error(f"Error parsing articles from page {page}: {e}")
            return []

    def _parse_article_card(self, container):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–∫–∏ —Å—Ç–∞—Ç—å–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ HTML itproger.com"""
        try:
            article = {}

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ span –≤–Ω—É—Ç—Ä–∏ —Å—Å—ã–ª–∫–∏
            title_elem = container.find('a')
            if title_elem:
                # –ò—â–µ–º span —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                title_span = title_elem.find('span')
                if title_span:
                    article['title'] = title_span.get_text().strip()
                else:
                    article['title'] = title_elem.get_text().strip()

                # –°—Å—ã–ª–∫–∞
                if title_elem.get('href'):
                    article['link'] = urljoin(self.base_url, title_elem['href'])
                else:
                    article['link'] = ""
            else:
                article['title'] = "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                article['link'] = ""

            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img_elem = container.find('img')
            if img_elem and img_elem.get('src'):
                article['image'] = urljoin(self.base_url, img_elem['src'])
            else:
                article['image'] = ""

            # –û–ø–∏—Å–∞–Ω–∏–µ - –≤—Ç–æ—Ä–æ–π span –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
            spans = container.find_all('span')
            if len(spans) >= 2:
                description = spans[1].get_text().strip()
                # –û—á–∏—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                description = re.sub(r'\s+', ' ', description)
                if len(description) > 200:
                    article['description'] = description[:200] + "..."
                else:
                    article['description'] = description
            else:
                article['description'] = ""

            # –ú–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∏ –¥–∞—Ç–∞)
            meta_div = container.find('div')
            if meta_div:
                meta_text = meta_div.get_text().strip()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                views_match = re.search(r'(\d+)\s*<', meta_text)
                if views_match:
                    views = views_match.group(1)
                else:
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                    views_match = re.search(r'(\d+)\s*–ø—Ä–æ—Å–º–æ—Ç—Ä', meta_text)
                    views = views_match.group(1) if views_match else "0"

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                date_match = re.search(r'(\d{1,2}\s+\w+\s+\d{4}\s+–≤\s+\d{1,2}:\d{2})', meta_text)
                if date_match:
                    date = date_match.group(1)
                else:
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã
                    date_match = re.search(r'(\d{2}\.\d{2}\.\d{4})', meta_text)
                    date = date_match.group(1) if date_match else ""

                article['meta'] = f"{views} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ ‚Ä¢ {date}" if date else f"{views} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"
                article['views'] = views
            else:
                article['meta'] = ""
                article['views'] = "0"

            logger.debug(f"Parsed article: {article['title'][:30]}...")
            return article

        except Exception as e:
            logger.error(f"Error parsing article card: {e}")
            return {}

    def get_total_pages(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        try:
            response = self.session.get(self.news_url)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –ò—â–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é itproger.com
            pagination = soup.find('div', class_='pagination')

            if pagination:
                # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                page_links = pagination.find_all('a')
                page_numbers = []

                for link in page_links:
                    try:
                        text = link.get_text().strip()
                        if text.isdigit():
                            page_numbers.append(int(text))
                    except ValueError:
                        continue

                if page_numbers:
                    max_page = max(page_numbers)
                    logger.info(f"Found {max_page} total pages from pagination")
                    return max_page

                # –ï—Å–ª–∏ –Ω–µ—Ç —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ (–∑–Ω–∞—á–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –º–Ω–æ–≥–æ)
                if pagination.find('span', string='...'):
                    logger.info("Found ellipsis in pagination, using 84 pages")
                    return 84

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            last_page_link = pagination.find('a', href=re.compile(r'page-84')) if pagination else None
            if last_page_link:
                logger.info("Found direct link to page 84")
                return 84

            # –ï—Å–ª–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 84
            logger.info("Using default 84 pages")
            return 84

        except Exception as e:
            logger.error(f"Error getting total pages: {e}")
            return 84

    def parse_full_article(self, article_url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç—å–∏"""
        try:
            response = self.session.get(article_url)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏ –Ω–∞ itproger.com
            content_elem = (soup.find('article') or
                            soup.find('div', class_='content') or
                            soup.find('div', class_=re.compile(r'post-content|article-content')))

            if not content_elem:
                return "üìù –ü–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è!"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –µ–≥–æ –≤ Markdown
            full_text = self._extract_formatted_text(content_elem)

            if not full_text:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥: —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                paragraphs = content_elem.find_all('p')
                full_text = "\n\n".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())

            return full_text if full_text else "üìù –ü–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ —Å–∫–æ—Ä–æ!"

        except Exception as e:
            logger.error(f"Error parsing full article: {e}")
            return "üìù –ü–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è!"

    def _extract_formatted_text(self, element):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Markdown"""
        try:
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for unwanted in element(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form']):
                unwanted.decompose()

            text_parts = []

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for elem in element.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'pre', 'code', 'ul', 'ol', 'blockquote']):
                if elem.name in ['h1', 'h2', 'h3', 'h4']:
                    text = elem.get_text().strip()
                    if text:
                        level = int(elem.name[1])
                        text_parts.append(f"{'#' * level} {text}\n\n")

                elif elem.name == 'p':
                    text = elem.get_text().strip()
                    if text and len(text) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã (–º–æ–≥—É—Ç –±—ã—Ç—å –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã–º–∏)
                        text_parts.append(f"{text}\n\n")

                elif elem.name == 'pre':
                    code_text = elem.get_text().strip()
                    if code_text:
                        text_parts.append(f"```\n{code_text}\n```\n\n")

                elif elem.name == 'code':
                    if elem.parent and elem.parent.name != 'pre':
                        text = elem.get_text().strip()
                        if text:
                            text_parts.append(f"`{text}` ")

                elif elem.name == 'ul':
                    items = []
                    for li in elem.find_all('li'):
                        text = li.get_text().strip()
                        if text:
                            items.append(f"‚Ä¢ {text}")
                    if items:
                        text_parts.append("\n".join(items) + "\n\n")

                elif elem.name == 'ol':
                    items = []
                    for i, li in enumerate(elem.find_all('li'), 1):
                        text = li.get_text().strip()
                        if text:
                            items.append(f"{i}. {text}")
                    if items:
                        text_parts.append("\n".join(items) + "\n\n")

                elif elem.name == 'blockquote':
                    text = elem.get_text().strip()
                    if text:
                        text_parts.append(f"> {text}\n\n")

            result = ''.join(text_parts)
            result = re.sub(r'\n{3,}', '\n\n', result).strip()

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º–∞
            if len(result) > 4000:
                result = result[:4000] + "...\n\nüìñ –ß–∏—Ç–∞—Ç—å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –Ω–∞ —Å–∞–π—Ç–µ"

            return result if result else None

        except Exception as e:
            logger.error(f"Error extracting formatted text: {e}")
            return None

    def test_parsing(self, page=1):
        """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        articles = self.parse_articles(page)
        print(f"=== Page {page} ===")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article['title']}")
            print(f"   Description: {article['description'][:50]}...")
            print(f"   Image: {article['image'][:50]}...")
            print(f"   Meta: {article['meta']}")
            print()
        return articles


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    parser = ITProgerParser()

    # –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    articles = parser.test_parsing(1)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
    total_pages = parser.get_total_pages()
    print(f"Total pages: {total_pages}")